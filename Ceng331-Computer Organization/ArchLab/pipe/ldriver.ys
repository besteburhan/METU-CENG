#######################################################################
# Test for copying block of size 63;
#######################################################################
	.pos 0
main:	irmovq Stack, %rsp  	# Set up stack pointer

	# Set up arguments for copy function and then invoke it
	irmovq $63, %rdx		# src and dst have 63 elements
	irmovq dest, %rsi	# dst array
	irmovq src, %rdi	# src array
	call ncopy		 
	halt			# should halt with num nonzeros in %rax
StartFun:
#Beste Burhan
#e2171395
#/* $begin ncopy-ys */
##################################################################
# ncopy.ys - Copy a src block of len words to dst.
# Return the number of positive words (>0) contained in src.
#
# Include your name and ID here.
#
# Describe how and why you modified the baseline code.
#
#		I used loop unrolling with 8 factor. Firstly, I check length >=8. If so, values are copied and
#		check whether values are greater than 0 or not. for length <8, I check length>=4 or length<4.
#		Then length<=2 or length=3.For length<=2 or length=3, I solved it as a straightforward way.
#		For length>=4, 4 values are copied and sent to len_l_4(length less than 4).
#		To avoid some control hazards(bubbles), I jump where that has more possibility to go into.
#		To avoid some data hazards, I try to add some instruction between irmovq
#		or some other instructions have data dependencies.
##################################################################
# Do not modify this portion
# Function prologue.
# %rdi = src, %rsi = dst, %rdx = len
ncopy:

##################################################################
# You can modify this portion
	# Loop header
	andq %rdx,%rdx		# len > 0?
	jg len_g_zero	# if so, goto len greater than zero(len_g_zero):
	ret
######
len_g_zero:
	irmovq $8,%r8
	subq %rdx,%r8			# 8-len>0
	jle len_ge_8			# len greater than or equal 8
	#len_l_8					# len less than 8
######
len_l_8:
	subq %rdx,%r8	#8-2len
	jg len_l_4				#len less than 4
	# len_ge_4				#len greater than or equal 4
######
len_ge_4:
	mrmovq (%rdi),%r11
	mrmovq 8(%rdi),%r12
	mrmovq 16(%rdi),%r13
	mrmovq 24(%rdi),%r14
	iaddq $32, %rdi		# src++
	rmmovq %r11, (%rsi)
	rmmovq %r12, 8(%rsi)
	rmmovq %r13, 16(%rsi)
	rmmovq %r14, 24(%rsi)
	iaddq $32, %rsi
	iaddq $-4,%rdx  #go to plus4
	pp4:
		andq %r11, %r11
		jle pp3
		iaddq $1,%rax
	pp3:
		andq %r12, %r12
		jle pp2
		iaddq $1,%rax
	pp2:
		andq %r13, %r13
		jle pp1
		iaddq $1,%rax
	pp1:
		andq %r14, %r14
		jle pp0
		iaddq $1,%rax
	pp0:
		irmovq $3,%r8
		andq %rdx,%rdx	#len>0
		jg len_l_4		# since length is already less than or equal to 3, i will sent it to len_l_4
		ret
######
len_l_4:
	subq %rdx,%r8		#8-3len
	jg len_le_2		# len less than or equal to 2
	# jmp len_g_2	# len greater than 2
########
len_g_2: 				#len=3
	mrmovq (%rdi),%r12
	mrmovq 8(%rdi),%r13
	mrmovq 16(%rdi),%r14
	rmmovq %r12, (%rsi)
	rmmovq %r13, 8(%rsi)
	rmmovq %r14, 16(%rsi)
	p3:
		andq %r12, %r12
		jle p2
		iaddq $1,%rax
	p2:
		andq %r13, %r13
		jle p1
		iaddq $1,%rax
	p1:
		andq %r14, %r14
		jle Done
		iaddq $1,%rax
		ret
########
len_le_2:			#len=1 or len=2
	mrmovq (%rdi),%r14
	rmmovq %r14, (%rsi)
	andq %r14, %r14
	jle c_len_le_2
	iaddq $1,%rax
c_len_le_2:
	iaddq $-1,%rdx
	andq %rdx,%rdx 	#len=1 or len=0
	jle Done			#len =0
	mrmovq 8(%rdi),%r14
	rmmovq %r14, 8(%rsi)
	andq %r14, %r14
	jle Done
	iaddq $1,%rax
	ret
######
len_ge_8:
	mrmovq (%rdi),%rcx
	mrmovq 8(%rdi),%rbx
	mrmovq 16(%rdi),%r9
	mrmovq 24(%rdi),%r10
	mrmovq 32(%rdi),%r11
	mrmovq 40(%rdi),%r12
	mrmovq 48(%rdi),%r13
	mrmovq 56(%rdi),%r14
	iaddq $64, %rdi		# src++
	rmmovq %rcx, (%rsi)
	rmmovq %rbx, 8(%rsi)
	rmmovq %r9, 16(%rsi)
	rmmovq %r10, 24(%rsi)
	rmmovq %r11, 32(%rsi)
	rmmovq %r12, 40(%rsi)
	rmmovq %r13, 48(%rsi)
	rmmovq %r14, 56(%rsi)
	iaddq $64, %rsi
	iaddq $-8,%rdx 	#go to plus 8
plus8:
	andq %rcx, %rcx
	jle plus7
	iaddq $1,%rax
plus7:
	andq %rbx, %rbx
	jle plus6
	iaddq $1,%rax
plus6:
	andq %r9, %r9
	jle plus5
	iaddq $1,%rax
plus5:
	andq %r10, %r10
	jle plus4
	iaddq $1,%rax
plus4:
	andq %r11, %r11
	jle plus3
	iaddq $1,%rax
plus3:
	andq %r12, %r12
	jle plus2
	iaddq $1,%rax
plus2:
	andq %r13, %r13
	jle plus1
	iaddq $1,%rax
plus1:
	andq %r14, %r14
	jle plus0
	iaddq $1,%rax
plus0:
	andq %rdx,%rdx	#len>0
	jg len_g_zero
##################################################################
# Do not modify the following section of code
# Function epilogue.
Done:
	ret
##################################################################
# Keep the following label at the end of your function
End:
#/* $end ncopy-ys */
EndFun:

###############################
# Source and destination blocks 
###############################
	.align 8
src:
	.quad -1
	.quad -2
	.quad 3
	.quad -4
	.quad 5
	.quad 6
	.quad 7
	.quad -8
	.quad 9
	.quad -10
	.quad 11
	.quad 12
	.quad -13
	.quad -14
	.quad 15
	.quad 16
	.quad -17
	.quad 18
	.quad -19
	.quad -20
	.quad 21
	.quad -22
	.quad -23
	.quad -24
	.quad -25
	.quad 26
	.quad 27
	.quad -28
	.quad -29
	.quad 30
	.quad 31
	.quad 32
	.quad -33
	.quad -34
	.quad 35
	.quad -36
	.quad 37
	.quad -38
	.quad 39
	.quad 40
	.quad 41
	.quad -42
	.quad -43
	.quad 44
	.quad -45
	.quad 46
	.quad -47
	.quad -48
	.quad 49
	.quad -50
	.quad -51
	.quad -52
	.quad 53
	.quad -54
	.quad -55
	.quad 56
	.quad 57
	.quad -58
	.quad 59
	.quad -60
	.quad 61
	.quad 62
	.quad 63
	.quad 0xbcdefa # This shouldn't get moved

	.align 16
Predest:
	.quad 0xbcdefa
dest:
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
Postdest:
	.quad 0xdefabc

.align 8
# Run time stack
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0

Stack:
